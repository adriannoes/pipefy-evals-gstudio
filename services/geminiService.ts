import { GoogleGenAI, Type } from "@google/genai";
import { AgentConfig, EvalResult, TestCase } from "../types";

// Initialize the client
// The API key must be provided in the environment variable API_KEY
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

/**
 * Simulates the "Pipefy Agent" performing a task.
 * For example: Classifying a ticket or extracting data.
 */
export const runAgentTask = async (
  input: string,
  config: AgentConfig
): Promise<{ text: string; latency: number }> => {
  const startTime = performance.now();

  try {
    const response = await ai.models.generateContent({
      model: config.model,
      contents: input,
      config: {
        systemInstruction: config.systemInstruction,
        temperature: config.temperature,
      },
    });

    const endTime = performance.now();
    return {
      text: response.text || "No response generated.",
      latency: Math.round(endTime - startTime),
    };
  } catch (error) {
    console.error("Agent Error:", error);
    return {
      text: "Error executing agent task.",
      latency: 0,
    };
  }
};

/**
 * Uses a stronger model (Gemini 3 Pro) to act as a Judge.
 * Compares the Agent's output against the Ground Truth (Expected Output).
 */
export const evaluateResponse = async (
  testCase: TestCase,
  actualOutput: string
): Promise<Omit<EvalResult, "latencyMs">> => {
  const prompt = `
    You are an expert AI Evaluator for Pipefy process automation.
    
    TASK:
    Evaluate the quality of the 'Actual Output' generated by an AI Agent against the 'Expected Output' provided by a human expert.
    
    CONTEXT:
    Input Data: "${testCase.input}"
    Expected Output (Ground Truth): "${testCase.expectedOutput}"
    Actual Output (AI Agent): "${actualOutput}"
    
    INSTRUCTIONS:
    1. Compare the semantic meaning. It does not need to be an exact string match, but the intent and key data points must match.
    2. Assign a score between 0.0 and 1.0 (1.0 being perfect).
    3. Provide a brief reasoning for your score.
    4. Determine if it is a PASS (Score >= 0.8) or FAIL.
  `;

  try {
    const response = await ai.models.generateContent({
      model: "gemini-3-pro-preview", // Use a smart model for judging
      contents: prompt,
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            score: { type: Type.NUMBER },
            reasoning: { type: Type.STRING },
            isPass: { type: Type.BOOLEAN },
          },
          required: ["score", "reasoning", "isPass"],
        },
      },
    });

    const jsonText = response.text;
    if (!jsonText) throw new Error("No evaluation response");

    const result = JSON.parse(jsonText);

    return {
      testCaseId: testCase.id,
      input: testCase.input,
      actualOutput: actualOutput,
      expectedOutput: testCase.expectedOutput,
      score: result.score,
      reasoning: result.reasoning,
      isPass: result.isPass,
    };
  } catch (error) {
    console.error("Evaluation Error:", error);
    // Fallback if judge fails
    return {
      testCaseId: testCase.id,
      input: testCase.input,
      actualOutput: actualOutput,
      expectedOutput: testCase.expectedOutput,
      score: 0,
      reasoning: "Evaluation failed due to API error.",
      isPass: false,
    };
  }
};
